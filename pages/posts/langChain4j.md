---
title: LangChain4j
categories: AI
tags:
  - Java
  - 后端
  - AI
---



:::warning
LangChain4j JDK不能低于17
:::


## 大模型部署

智能应用就是在传统软件的基础上接入大模型，所以，我们要完成智能应用的开发，首先得把大模型这种软件部署起来，而大模型的部署会有两种方式，自己部署、他人部署。自己部署大模型自己直接用，他人部署的大模型我们掏钱用。接下来我们分别聊一聊这两种方式的优缺点。

:::tip
自己部署：

云服务器部署：  
- 优势：前期成本低，维护简单  
- 劣势：数据不安全，长期使用成本高  

本地机器部署：  
- 优势：数据安全，长期使用成本低  
- 劣势：初期成本高，维护困难  
:::

:::tip
他人部署：  
- 优势：无需部署  
- 劣势：数据不安全，长期使用成本高  
:::

首先看自己部署，我们自己在部署大模型的时候，也会有两种方式，一种是在云端部署，另外一种是在本地机房部署。在云端部署的优点是前期部署成本低，维护简单，比如你去阿里云租服务器，按天收费，我们可以花很少的费用，就能快速上手，并且像阿里云这样的平台，服务器维护成本也是很低的。但缺点就是数据不安全，因为使用别人提供的服务器，数据都得从这个服务器过一圈，数据自然就不安全了；还有就是长期使用成本高，虽然阿里云租服务器每天的收费看起来不算贵，但是你只要用一天，就得付一天钱，时间长了，这个费用其实还是蛮高的。

我们自己部署的另外一种方式就是部署在本地机房中，这种方式相比较云端部署，它的优势是数据安全，毕竟自己的服务器嘛，数据并不会向外部暴露，还有就是长期成本低，因为是一次性投入，时间越长，平均成本就越低。反过来，它的缺点是初期成本高，买服务器的钱是一次性支付的，还有就是维护困难一些，因为自己买的服务器，所有的维护工作都需要自己来做。

我们再来看他人部署，都有谁会帮我们部署大模型呢？这样的好事者有很多，常见的比如有阿里云百炼、百度智能云、硅基流动、火山引擎等等。它们部署好的大模型，我们怎么用呢？常规思路，使用他们提供的API接口使用，当然了，你使用的时候，它会按照流量进行收费的，毕竟天下没有免费的午餐。使用这些平台的大模型，优点是我们自己无需部署，缺点是数据不安全、长期使用成本高。

ollama、LM Studio可以
一键下载、运行大模型

![](https://zzyang.oss-cn-hangzhou.aliyuncs.com/img/20251229235403618.png)

### ollama本机部署大模型并使用

#### 安装ollama

Ollama的官网是:https://ollama.com/

大家打开后，首页就有一个下载按钮，你只要点击一下download，选择对应的操作系统，就可以下载对应版本的ollama了。

:::info
ollama安装完毕后，会自动的配置系统环境变量，因此接下来我们就可以直接执行ollama的命令去部署大模型了，如果有同学将来执行命令的时候报错，请记得检查一下你的环境变量, 可以手动的配置一下
:::



#### 部署大模型

ollama官网上给出了很多大模型，大家可以根据自己的需求选择对应的大模型安装，这里咱们安装qwen3系列模型，首先点击导航栏的Models来到模型列表

然后点击模型列表中的qwen3, 来到qwen3详情页面


这里提供了不同参数规模的qwen3模型，由于参数规模越大，对电脑的配置要求越高，为了照顾到大部分同学的电脑，这里我们部署最小参数规模的大模型qwen3:0.6b来部署，点击模型的名称，来到该模型的详情页面，并赋值右上角的命令。
![](https://zzyang.oss-cn-hangzhou.aliyuncs.com/img/20251230000152165.png)

找到安装位置`D:\app\Ollama`，进行cmd命令操作

打开命令行提示符窗口，执行这个命令，命令执行的过程中，会自动下载qwen3:0.6b这个模型到电脑本地，并自动的运行起来，命令行提示符窗口如果自动进入到聊天界面，证明模型部署正确。
```bash
ollama run qwen3:0.6b
```


接下来你就可以跟本地部署的大模型进行对话了，输入问题敲回车即可

如果不想继续与大模型对话，可以使用 `/bye` 命令退出聊天界面

如果想继续与大模型聊天，可以再次执行 ollama run qwen3:0.6b, 这一次再执行的时候，由于本地已经有了这个大模型并运行起来了，所以不会再次下载，而是直接进入聊天界面。

---

**有关ollama提供的命令有很多，如下**


#### **一、基础操作指令**

| 指令                   | 功能                             | 示例                  |
| ---------------------- | -------------------------------- | --------------------- |
| `ollama run <模型名>`  | 运行指定模型（自动下载若不存在） | `ollama run llama3`   |
| `ollama list`          | 查看本地已下载的模型列表         | `ollama list`         |
| `ollama pull <模型名>` | 手动下载模型                     | `ollama pull mistral` |
| `ollama rm <模型名>`   | 删除本地模型                     | `ollama rm llama2`    |
| `ollama help`          | 查看帮助文档                     | `ollama help`         |

#### **二、模型交互指令**

##### **1. 直接对话**

```bash
ollama run llama3 "用中文写一首关于秋天的诗"
```

##### **2. 进入交互模式**

```bash
ollama run llama3
# 进入后输入内容，按 Ctrl+D 或输入 `/bye` 退出
```

##### **3. 从文件输入**

```bash
ollama run llama3 --file input.txt
```

##### **4. 流式输出控制**

| 参数           | 功能         | 示例                             |
| -------------- | ------------ | -------------------------------- |
| `--verbose`    | 显示详细日志 | `ollama run llama3 --verbose`    |
| `--nowordwrap` | 禁用自动换行 | `ollama run llama3 --nowordwrap` |

#### **三、模型管理**

##### **1. 自定义模型配置（Modelfile）**

创建 `Modelfile` 文件：

```bash
FROM llama3  # 基础模型
PARAMETER temperature 0.7  # 控制随机性（0-1）
PARAMETER num_ctx 4096     # 上下文长度
SYSTEM """ 你是一个严谨的学术助手，回答需引用论文来源。"""                # 系统提示词
```

构建自定义模型：

```bash
ollama create my-llama3 -f Modelfile
ollama run my-llama3
```

##### **2. 查看模型信息**

```bash
ollama show <模型名> --modelfile  # 查看模型配置
ollama show <模型名> --parameters # 查看运行参数
```

#### **四、高级功能**

##### **1. API 调用**

启动 API 服务

```bash
ollama serve
```

通过 HTTP 调用

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "llama3",
  "prompt": "你好",
  "stream": false
}'
```

##### **2. GPU 加速配置**

```bash
# 指定显存分配比例（50%）
ollama run llama3 --num-gpu 50
```


---






#### 调用大模型

文档地址：https://ollama.com/blog/thinking

ollama平台也开放了API，程序员可以使用发送http请求的方式调用本地部署的大模型，这里咱们借助于Apifox工具调用大模型

本机ollama默认占用的端口为11434，调用大模型时发送的请求方式必须是post，请求数据必须是json格式，具体样例如下：

```json
POST
http://localhost:11434/api/chat
{
    "model": "qwen3:0.6b",
    "messages": [
        {
            "role": "user",
            "content": "你是谁?"
        }
    ],
    "think": true,
    "stream": false
}
```
![](https://zzyang.oss-cn-hangzhou.aliyuncs.com/img/20251230002203622.png)